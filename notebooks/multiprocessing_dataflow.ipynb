{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi, I saw in the code of `image_classifier` there is an attempt to parallelize image augmentation:  https://github.com/paris-saclay-cds/ramp-workflow/blob/master/rampwf/workflows/image_classifier.py#L244\n",
    "\n",
    "I tested this (on Python 3) and the problem of pickling can be solved by a simple tweak : https://gist.github.com/vfdev-5/9ae8fe64cb6933f6c94614d335b67d3d#file-parallized_data_loading_augmentation-py\n",
    "This allows to run a little bit faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "# sys.path.append(\"../submissions/keras_cnns_pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rampwf.workflows.image_classifier import BatchGeneratorBuilder\n",
    "from problem import get_cv, get_train_data\n",
    "# from image_preprocessor import transform, transform_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "image_preprocessor = imp.load_source('image_preprocessor', '../submissions/keras_cnns_pretrained/image_preprocessor.py')\n",
    "transform = image_preprocessor.transform\n",
    "transform_test = image_preprocessor.transform_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 403\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_X_train, y_train = get_train_data(path=\"..\")\n",
    "cv = list(get_cv(folder_X_train, y_train))\n",
    "\n",
    "for fold_i, (train_is, valid_is) in enumerate(cv):\n",
    "\n",
    "    folder, X_train = folder_X_train\n",
    "\n",
    "    gen_builder = BatchGeneratorBuilder(X_array=X_train[train_is], y_array=y_train[train_is], \n",
    "                                transform_img=transform, transform_test_img=transform_test, \n",
    "                                folder=folder, \n",
    "                                chunk_size=batch_size*5, \n",
    "                                n_classes=n_classes, n_jobs=1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from joblib import delayed\n",
    "from joblib import Parallel\n",
    "\n",
    "from rampwf.workflows.image_classifier import _chunk_iterator, _to_categorical, get_nb_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def local_get_generator(self, indices=None, batch_size=256):\n",
    "#     if indices is None:\n",
    "#         indices = np.arange(self.nb_examples)\n",
    "#     # Infinite loop, as required by keras `fit_generator`.\n",
    "#     # However, as we provide the number of examples per epoch\n",
    "#     # and the user specifies the total number of epochs, it will\n",
    "#     # be able to end.\n",
    "#     while True:\n",
    "#         if self.shuffle:\n",
    "#             np.random.shuffle(indices)\n",
    "#         it = _chunk_iterator(\n",
    "#             X_array=self.X_array[indices], folder=self.folder,\n",
    "#             y_array=self.y_array[indices], chunk_size=self.chunk_size,\n",
    "#             n_jobs=self.n_jobs)\n",
    "#         for X, y in it:\n",
    "\n",
    "#             # 1) Preprocessing of X and y\n",
    "#             X = Parallel(n_jobs=self.n_jobs, backend='multiprocessing')(\n",
    "#                      delayed(self.transform_img)(x) for x in X)\n",
    "#             # X = np.array([self.transform_img(x) for x in X])\n",
    "#             # X is a list of numpy arrrays at this point, convert it to a\n",
    "#             # single numpy array.\n",
    "#             try:\n",
    "#                 X = [x[np.newaxis, :, :, :] for x in X]\n",
    "#             except IndexError:\n",
    "#                 # single channel\n",
    "#                 X = [x[np.newaxis, np.newaxis, :, :] for x in X]\n",
    "#             X = np.concatenate(X, axis=0)\n",
    "#             X = np.array(X, dtype='float32')\n",
    "#             # Convert y to onehot representation\n",
    "#             y = _to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "#             # 2) Yielding mini-batches\n",
    "#             for i in range(0, len(X), batch_size):\n",
    "#                 yield X[i:i + batch_size], y[i:i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "def _chunk_iterator2(parallel, X_array, folder, y_array=None, chunk_size=1024):\n",
    "    from skimage.io import imread\n",
    "    for i in range(0, len(X_array), chunk_size):\n",
    "        X_chunk = X_array[i:i + chunk_size]\n",
    "        filenames = [os.path.join(folder, '{}'.format(x)) for x in X_chunk]\n",
    "        X = parallel(delayed(imread)(filename) for filename in filenames)\n",
    "        if y_array is not None:\n",
    "            y = y_array[i:i + chunk_size]\n",
    "            yield X, y\n",
    "        else:\n",
    "            yield X\n",
    "\n",
    "\n",
    "def local_get_generator2(self, indices=None, batch_size=256):\n",
    "    if indices is None:\n",
    "        indices = np.arange(self.nb_examples)\n",
    "    # Infinite loop, as required by keras `fit_generator`.\n",
    "    # However, as we provide the number of examples per epoch\n",
    "    # and the user specifies the total number of epochs, it will\n",
    "    # be able to end.\n",
    "\n",
    "    with Parallel(n_jobs=self.n_jobs, backend='threading') as parallel:\n",
    "        while True:\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "            it = _chunk_iterator2(parallel,\n",
    "                X_array=self.X_array[indices], folder=self.folder,\n",
    "                y_array=self.y_array[indices], chunk_size=self.chunk_size)\n",
    "\n",
    "            for X, y in it:\n",
    "                # 1) Preprocessing of X and y\n",
    "                X = parallel(delayed(self.transform_img)(x) for x in X)\n",
    "                # X = np.array([self.transform_img(x) for x in X])\n",
    "                # X is a list of numpy arrrays at this point, convert it to a\n",
    "                # single numpy array.\n",
    "                try:\n",
    "                    X = [x[np.newaxis, :, :, :] for x in X]\n",
    "                except IndexError:\n",
    "                    # single channel\n",
    "                    X = [x[np.newaxis, np.newaxis, :, :] for x in X]\n",
    "                X = np.concatenate(X, axis=0)\n",
    "                X = np.array(X, dtype='float32')\n",
    "                # Convert y to onehot representation\n",
    "                y = _to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "                # 2) Yielding mini-batches\n",
    "                for i in range(0, len(X), batch_size):\n",
    "                    yield X[i:i + batch_size], y[i:i + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_builder._get_generator = types.MethodType(local_get_generator2, gen_builder)\n",
    "# gen_builder._get_generator = types.MethodType(local_get_generator, gen_builder)\n",
    "gen_builder.n_jobs = 8\n",
    "gen_builder.shuffle = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "gen_builder.chunk_size = batch_size * 5\n",
    "gen_train, gen_valid, nb_train, nb_valid = gen_builder.get_train_valid_generators(batch_size=batch_size, valid_ratio=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "max_counter = 50\n",
    "for X, y in gen_train:\n",
    "    max_counter -= 1\n",
    "    if max_counter == 0:\n",
    "        break\n",
    "        \n",
    "print(type(X), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 224, 224, 3), (64, 403))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check with Keras Model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Activation, BatchNormalization\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet')\n",
    "    for l in vgg16.layers:\n",
    "        l.trainable = False\n",
    "    inp = Input((224, 224, 3))\n",
    "    x = vgg16(inp)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(4096, activation='linear', name='fc1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(4096, activation='linear', name='fc2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    out = Dense(403, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', optimizer=Adam(lr=0.001),\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 24/511 [>.............................] - ETA: 219s - loss: 7.0643 - acc: 0.0970"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-028ca5f702e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_nb_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     verbose=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_model().fit_generator(\n",
    "    gen_train,\n",
    "    steps_per_epoch=get_nb_minibatches(nb_train, batch_size),\n",
    "    epochs=1,\n",
    "    max_queue_size=batch_size,    \n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=get_nb_minibatches(nb_valid, batch_size),\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit_generator in module keras.engine.training:\n",
      "\n",
      "fit_generator(self, generator, steps_per_epoch, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      "    Fits the model on data yielded batch-by-batch by a Python generator.\n",
      "    \n",
      "    The generator is run in parallel to the model, for efficiency.\n",
      "    For instance, this allows you to do real-time data augmentation\n",
      "    on images on CPU in parallel to training your model on GPU.\n",
      "    \n",
      "    The use of `keras.utils.Sequence` guarantees the ordering\n",
      "    and guarantees the single use of every input per epoch when\n",
      "    using `use_multiprocessing=True`.\n",
      "    \n",
      "    # Arguments\n",
      "        generator: A generator or an instance of Sequence (keras.utils.Sequence)\n",
      "                object in order to avoid duplicate data\n",
      "                when using multiprocessing.\n",
      "            The output of the generator must be either\n",
      "            - a tuple (inputs, targets)\n",
      "            - a tuple (inputs, targets, sample_weights).\n",
      "            All arrays should contain the same number of samples.\n",
      "            The generator is expected to loop over its data\n",
      "            indefinitely. An epoch finishes when `steps_per_epoch`\n",
      "            batches have been seen by the model.\n",
      "        steps_per_epoch: Total number of steps (batches of samples)\n",
      "            to yield from `generator` before declaring one epoch\n",
      "            finished and starting the next epoch. It should typically\n",
      "            be equal to the number of unique samples if your dataset\n",
      "            divided by the batch size.\n",
      "        epochs: Integer, total number of iterations on the data.\n",
      "        verbose: Verbosity mode, 0, 1, or 2.\n",
      "        callbacks: List of callbacks to be called during training.\n",
      "        validation_data: This can be either\n",
      "            - a generator for the validation data\n",
      "            - a tuple (inputs, targets)\n",
      "            - a tuple (inputs, targets, sample_weights).\n",
      "        validation_steps: Only relevant if `validation_data`\n",
      "            is a generator. Total number of steps (batches of samples)\n",
      "            to yield from `generator` before stopping.\n",
      "        class_weight: Dictionary mapping class indices to a weight\n",
      "            for the class.\n",
      "        max_queue_size: Maximum size for the generator queue\n",
      "        workers: Maximum number of processes to spin up\n",
      "            when using process based threading\n",
      "        use_multiprocessing: If True, use process based threading.\n",
      "            Note that because\n",
      "            this implementation relies on multiprocessing,\n",
      "            you should not pass\n",
      "            non picklable arguments to the generator\n",
      "            as they can't be passed\n",
      "            easily to children processes.\n",
      "        shuffle: Whether to shuffle the data at the beginning of each\n",
      "            epoch. Only used with instances of `Sequence` (\n",
      "            keras.utils.Sequence).\n",
      "        initial_epoch: Epoch at which to start training\n",
      "            (useful for resuming a previous training run)\n",
      "    \n",
      "    # Returns\n",
      "        A `History` object.\n",
      "    \n",
      "    # Example\n",
      "    \n",
      "    ```python\n",
      "        def generate_arrays_from_file(path):\n",
      "            while 1:\n",
      "                f = open(path)\n",
      "                for line in f:\n",
      "                    # create numpy arrays of input data\n",
      "                    # and labels, from each line in the file\n",
      "                    x1, x2, y = process_line(line)\n",
      "                    yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
      "                f.close()\n",
      "    \n",
      "        model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      "                            steps_per_epoch=10000, epochs=10)\n",
      "    ```\n",
      "    \n",
      "    # Raises\n",
      "        ValueError: In case the generator yields\n",
      "            data in an invalid format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Model.fit_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.fit??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
